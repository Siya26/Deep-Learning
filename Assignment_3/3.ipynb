{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.sources\") as f:\n",
    "    train_sources = f.readlines()\n",
    "with open(\"train.targets\") as f:\n",
    "    train_targets = f.readlines()\n",
    "\n",
    "train_sources_tokenized = []\n",
    "for i in range(len(train_sources)):\n",
    "    train_sources_tokenized.append(train_sources[i].replace(\"\\n\", \"\").split(\" \"))\n",
    "\n",
    "train_targets_tokenized = []\n",
    "for i in range(len(train_targets)):\n",
    "    train_targets_tokenized.append(train_targets[i].replace(\"\\n\", \"\").split(\" \"))\n",
    "\n",
    "\n",
    "train_sources_vocabulary = []\n",
    "for i in range(len(train_sources_tokenized)):\n",
    "    for j in range(len(train_sources_tokenized[i])):\n",
    "        if train_sources_tokenized[i][j] not in train_sources_vocabulary:\n",
    "            train_sources_vocabulary.append(train_sources_tokenized[i][j])\n",
    "\n",
    "train_targets_vocabulary = []\n",
    "for i in range(len(train_targets_tokenized)):\n",
    "    for j in range(len(train_targets_tokenized[i])):\n",
    "        if train_targets_tokenized[i][j] not in train_targets_vocabulary:\n",
    "            train_targets_vocabulary.append(train_targets_tokenized[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500  \n",
    "train_sources_sequences = np.zeros((len(train_sources_tokenized), MAX_SEQUENCE_LENGTH))\n",
    "train_targets_sequences = np.zeros((len(train_targets_tokenized), MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "for i in range(len(train_sources_tokenized)):\n",
    "    for j in range(min(MAX_SEQUENCE_LENGTH, len(train_sources_tokenized[i]))):\n",
    "        token = train_sources_tokenized[i][j]\n",
    "        if token in train_sources_vocabulary:\n",
    "            train_sources_sequences[i, j] = train_sources_vocabulary.index(token)\n",
    "        else:\n",
    "            train_sources_sequences[i, j] = train_sources_vocabulary.index('<OOV>')\n",
    "\n",
    "for i in range(len(train_targets_tokenized)):\n",
    "    for j in range(min(MAX_SEQUENCE_LENGTH, len(train_targets_tokenized[i]))):\n",
    "        token = train_targets_tokenized[i][j]\n",
    "        if token in train_targets_vocabulary:\n",
    "            train_targets_sequences[i, j] = train_targets_vocabulary.index(token)\n",
    "        else:\n",
    "            train_targets_sequences[i, j] = train_targets_vocabulary.index('<OOV>')\n",
    "\n",
    "X = torch.from_numpy(train_sources_sequences).long()\n",
    "Y = torch.from_numpy(train_targets_sequences).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x) \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        hidden = (hidden[::2, :, :] + hidden[1::2, :, :]) / 2\n",
    "        cell = (cell[::2, :, :] + cell[1::2, :, :]) / 2\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "    \n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.v = torch.nn.Parameter(torch.rand(hid_dim))\n",
    "        self.v.data.normal_(mean=0, std=1. / np.sqrt(self.v.size(0)))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        seq_length = encoder_outputs.shape[0]\n",
    "        hidden_size_adjusted = encoder_outputs.shape[2] // 2\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, seq_length, hidden_size_adjusted)\n",
    "\n",
    "        attention_scores = torch.tanh(torch.sum(hidden * encoder_outputs, dim=2))  # (batch_size, seq_length)\n",
    "        attention_weights = F.softmax(attention_scores, dim=1).unsqueeze(1)  # (batch_size, 1, seq_length)\n",
    "        return attention_weights\n",
    "\n",
    "    \n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(input_size, embedding_size)\n",
    "        self.lstm = torch.nn.LSTM(embedding_size+hidden_size*2, hidden_size, num_layers, dropout=dropout)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.fc = torch.nn.Linear(hidden_size, input_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell, encoder_outputs):\n",
    "        x = x.unsqueeze(0)  # (batch_size) -> (1, batch_size)\n",
    "        embedded = self.embedding(x)  # (1, batch_size, embedding_size)\n",
    "\n",
    "        attention = self.attention(hidden[-1], encoder_outputs)  # (batch_size, 1, seq_length)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # (seq_length, batch_size, hidden_size*2) -> (batch_size, seq_length, hidden_size*2)\n",
    "        weights = torch.bmm(attention, encoder_outputs)  # (batch_size, 1, hidden_size*2)\n",
    "        weights = weights.transpose(0, 1)  # (1, batch_size, hidden_size*2)\n",
    "\n",
    "        dec_input = torch.cat((embedded, weights), dim=2)  # (1, batch_size, embedding_size+hidden_size*2)\n",
    "        output, (hidden, cell) = self.lstm(dec_input)  # (1, batch_size, hidden_size) ,(num_layers, batch_size, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "        prediction = self.fc(output.squeeze(0))  # (batch_size, input_size)\n",
    "        return prediction, hidden, cell # (batch_size, input_size)\n",
    "    \n",
    "class Seq2Seq(torch.nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        batch_size = source.shape[1]    # (seq_length, batch_size)\n",
    "        seq_length = target.shape[0]    # (seq_length, batch_size)\n",
    "        input_size = self.decoder.input_size\n",
    "\n",
    "        outputs = torch.zeros(seq_length, batch_size, input_size).to(self.device)  \n",
    "        encoder_outputs, hidden, cell = self.encoder(source)  # (seq_length, batch_size, hidden_size*2)\n",
    "        batch = target[0]  \n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            output, hidden, cell = self.decoder(batch, hidden, cell, encoder_outputs)  # (batch_size, input_size), (num_layers, batch_size, hidden_size), (num_layers, batch_size, hidden_size)\n",
    "            outputs[i] = output\n",
    "            batch = target[i] if random.random() < teacher_forcing_ratio else output.argmax(1)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        source = batch[0].to(device)\n",
    "        target = batch[1].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(source, target, 0.5)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        target = target[1:].view(-1)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "INPUT_ENC_DIM = len(train_sources_vocabulary)\n",
    "INPUT_DEC_DIM = len(train_targets_vocabulary)\n",
    "OUTPUT_DIM = len(train_targets_vocabulary)\n",
    "HIDDEN_DIM = 512\n",
    "EMBEDDING_DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "BATCH_SIZE = 32\n",
    "NUM_STEPS = 20000\n",
    "CLIP = 1\n",
    "LEARNING_RATE = 1e-4\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X, Y)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "NUM_EPOCHS = math.ceil(NUM_STEPS / len(train_dataloader))\n",
    "\n",
    "encoder = Encoder(INPUT_ENC_DIM, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "decoder = Decoder(INPUT_DEC_DIM, OUTPUT_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=502)\n",
    "\n",
    "for i in range(10):\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP, DEVICE)\n",
    "    print('Epoch: {}, Train Loss: {}'.format(i, train_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
